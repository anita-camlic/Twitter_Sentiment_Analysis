{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\drewh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this do download stopwords if you haven't done this before\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in dataframe, using special encoder parameter to save it correctly\n",
    "df = pd.read_csv('twitter_data.csv',encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "df = df.rename(columns={\"tweet_text\": \"tweet\", \"emotion_in_tweet_is_directed_at\": \"subject\", \"is_there_an_emotion_directed_at_a_brand_or_product\": \"emotion\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = df.loc[(df['emotion'] == 'Positive emotion' ) | (df['emotion'] == 'Negative emotion' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drewh\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\drewh\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "df_binary.loc[df_binary['emotion'] == 'Positive emotion' , 'emotion_label'] = 0\n",
    "df_binary.loc[df_binary['emotion'] == 'Negative emotion' , 'emotion_label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to label new column based on the product category \n",
    "\n",
    "def label_brand(row):\n",
    "    if row['subject'] == 'iPad':\n",
    "        return 'Apple'\n",
    "    elif row['subject'] == 'Apple':\n",
    "        return 'Apple'\n",
    "    elif row['subject'] == 'iPad or iPhone App':\n",
    "        return 'Apple'\n",
    "    elif row['subject'] == 'iPhone':\n",
    "        return 'Apple'\n",
    "    elif row['subject'] == 'Other Apple product or service':\n",
    "        return 'Apple'\n",
    "    elif row['subject'] == 'Google':\n",
    "        return 'Google'\n",
    "    elif row['subject'] == 'Other Google product or service':\n",
    "        return 'Google'\n",
    "    elif row['subject'] == 'Android App':\n",
    "        return 'Google'\n",
    "    elif row['subject'] == 'Android':\n",
    "        return 'Google'\n",
    "    elif row['subject'] == np.nan: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-3c2e651705a5>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_binary['company'] = df_binary.apply(lambda row: label_brand(row), axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_binary['company'] = df_binary.apply(lambda row: label_brand(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_words = ['apple','ipad','iphone','itunes']\n",
    "google_words = ['google','android']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary.loc[(df_binary['subject'].isna()) & (df_binary['tweet'].str.contains('|'.join(apple_words))),'company'] = 'Apple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary.loc[(df_binary['subject'].isna()) & (df_binary['tweet'].str.contains('|'.join(google_words))),'company'] = 'Google'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-7638a198be87>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_binary.dropna(subset=['company'],inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_binary.dropna(subset=['company'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_binary['tweet'], df_binary['emotion_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_baseline = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",stop_words = og_stopwords)\n",
    "baseline_vectorized = tfidf_baseline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446448495185305"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier on baseline_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, baseline_vectorized, y_train)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8589743589743589\n"
     ]
    }
   ],
   "source": [
    "# finding training score \n",
    "baseline_model.fit(baseline_vectorized,y_train)\n",
    "baseline_train_score = baseline_model.score(baseline_vectorized,y_train)\n",
    "print(baseline_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating recall\n",
    "y_pred_baseline = baseline_model.predict(baseline_vectorized)\n",
    "recall_score(y_train, y_pred_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_1 = TfidfVectorizer(token_pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\",stop_words = og_stopwords)\n",
    "vectorized_1 = tfidf_1.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8352172831609991"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier on basekube_vectorized and y_train\n",
    "cv1 = cross_val_score(dt,vectorized_1, y_train)\n",
    "cv1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996229260935143\n"
     ]
    }
   ],
   "source": [
    "# finding training score \n",
    "dt.fit(vectorized_1,y_train)\n",
    "dt1_score = dt.score(vectorized_1,y_train)\n",
    "print(dt1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating recall\n",
    "y_pred_dt = dt.predict(baseline_vectorized)\n",
    "recall_score(y_train, y_pred_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_2 = TfidfVectorizer(token_pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\",stop_words = og_stopwords)\n",
    "vectorized_2 = tfidf_2.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8680247308389297"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the random forest\n",
    "cv2 = cross_val_score(rf,vectorized_2, y_train)\n",
    "cv2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996229260935143\n"
     ]
    }
   ],
   "source": [
    "# finding training score \n",
    "rf.fit(vectorized_2,y_train)\n",
    "rf1_score = rf.score(vectorized_2,y_train)\n",
    "print(rf1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9976470588235294"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating recall\n",
    "y_pred_rf = rf.predict(vectorized_2)\n",
    "recall_score(y_train, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3: Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_3 = TfidfVectorizer(token_pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                          stop_words = og_stopwords,\n",
    "                          ngram_range = (1,2),\n",
    "                          min_df= 5\n",
    "                         )\n",
    "vectorized_3 = tfidf_3.fit_transform(X_train)\n",
    "\n",
    "# five is the best input for our min_df parameter\n",
    "# (1,2) is the best input for our ngram_range parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8650079948832747"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the random forest\n",
    "cv3 = cross_val_score(rf,vectorized_3, y_train)\n",
    "cv3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996229260935143\n"
     ]
    }
   ],
   "source": [
    "# finding training score \n",
    "rf.fit(vectorized_3, y_train)\n",
    "rf2_score = rf.score(vectorized_3, y_train)\n",
    "print(rf2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9976470588235294"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating recall\n",
    "y_pred_rf2 = rf.predict(vectorized_3)\n",
    "recall_score(y_train, y_pred_rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the shape of our tfidf sparse matrix\n",
    "# pd.DataFrame.sparse.from_spmatrix(vectorized_3, columns=tfidf_3.get_feature_names()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_4 = TfidfVectorizer(token_pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                          stop_words = og_stopwords,\n",
    "                          ngram_range = (1,2),\n",
    "                          min_df= 5\n",
    "                         )\n",
    "vectorized_4 = tfidf_4.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for random forest parameters\n",
    "grid_rf1 = {\"n_estimators\":[100, 200, 300],\n",
    "           \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "           \"max_features\": [\"sqrt\", \"auto\"],\n",
    "           \"max_depth\" : [10,20,30,None],\n",
    "            \"min_samples_leaf\" : [1,2,4],\n",
    "            \"min_samples_split\": [2,5,10]\n",
    "            }\n",
    "GS_rf1 = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=grid_rf1,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GS_rf1.fit(vectorized_4,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GS_rf1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'criterion': 'gini',\n",
    "\n",
    "'max_depth': None,\n",
    "\n",
    "'max_features': 'auto',\n",
    "\n",
    "'min_samples_leaf': 1,\n",
    "\n",
    "'min_samples_split': 2,\n",
    "\n",
    "'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GS_rf1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8695299008634473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for random forest parameters\n",
    "grid_rf2 = {\"n_estimators\":[150, 200, 250],\n",
    "           \"criterion\": [\"gini\"],\n",
    "           \"max_features\": [\"auto\"],\n",
    "           \"max_depth\" : [None],\n",
    "            \"min_samples_leaf\" : [1],\n",
    "            \"min_samples_split\": [1,2,3]\n",
    "            }\n",
    "GS_rf2 = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=grid_rf2,\n",
    "    n_jobs=-1,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   13.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   13.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini'], 'max_depth': [None],\n",
       "                         'max_features': ['auto'], 'min_samples_leaf': [1],\n",
       "                         'min_samples_split': [1, 2, 3],\n",
       "                         'n_estimators': [150, 200, 250]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_rf2.fit(vectorized_4,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_rf2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8710414667945848"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_rf2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for random forest parameters\n",
    "grid_rf3 = {\"n_estimators\":[175, 200, 225],\n",
    "           \"criterion\": [\"gini\"],\n",
    "           \"max_features\": [\"auto\"],\n",
    "           \"max_depth\" : [None],\n",
    "            \"min_samples_leaf\" : [1],\n",
    "            \"min_samples_split\": [3,4,5]\n",
    "            }\n",
    "GS_rf3 = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=grid_rf3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    8.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    8.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini'], 'max_depth': [None],\n",
       "                         'max_features': ['auto'], 'min_samples_leaf': [1],\n",
       "                         'min_samples_split': [3, 4, 5],\n",
       "                         'n_estimators': [175, 200, 225]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_rf3.fit(vectorized_4,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_rf3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8676480830046549"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_rf3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf3 = RandomForestClassifier(n_estimators=225,\n",
    "                             criterion=\"gini\",\n",
    "                             max_depth=None,\n",
    "                             min_samples_split=3,\n",
    "                             min_samples_leaf=1,\n",
    "                             max_features=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8691575169669189"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the random forest\n",
    "cv4 = cross_val_score(rf3, vectorized_4, y_train)\n",
    "cv4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996229260935143\n"
     ]
    }
   ],
   "source": [
    "# finding training score \n",
    "rf3.fit(vectorized_4, y_train)\n",
    "rf3_score = rf3.score(vectorized_4, y_train)\n",
    "print(rf3_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9976470588235294"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating recall\n",
    "y_pred_rf3 = rf3.predict(vectorized_4)\n",
    "recall_score(y_train, y_pred_rf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Additional Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_eval = pd.concat([X_train, y_train], axis=1)\n",
    "word_eval_pos = word_eval.loc[word_eval[\"emotion_label\"] == 0]\n",
    "word_eval_neg = word_eval.loc[word_eval[\"emotion_label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pos = TfidfVectorizer(token_pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                          stop_words = og_stopwords,\n",
    "                          ngram_range = (1,2), min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vec = tfidf_pos.fit_transform(word_eval_pos[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_matrix = pd.DataFrame.sparse.from_spmatrix(pos_vec, columns=tfidf_pos.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>aclu</th>\n",
       "      <th>action</th>\n",
       "      <th>action link</th>\n",
       "      <th>actually</th>\n",
       "      <th>ad</th>\n",
       "      <th>adoption</th>\n",
       "      <th>agree</th>\n",
       "      <th>almost</th>\n",
       "      <th>already</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>years every</th>\n",
       "      <th>years time</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yes gowalla</th>\n",
       "      <th>yet</th>\n",
       "      <th>yrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2227 rows Ã— 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able      aclu  action  action link  actually   ad  adoption  agree  \\\n",
       "0      0.0  0.275287     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "1      0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "2      0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "3      0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "4      0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "...    ...       ...     ...          ...       ...  ...       ...    ...   \n",
       "2222   0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "2223   0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "2224   0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "2225   0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "2226   0.0  0.000000     0.0          0.0       0.0  0.0       0.0    0.0   \n",
       "\n",
       "      almost   already  ...  yeah  year     years  years every  years time  \\\n",
       "0        0.0  0.000000  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "1        0.0  0.000000  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "2        0.0  0.000000  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "3        0.0  0.000000  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "4        0.0  0.000000  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "...      ...       ...  ...   ...   ...       ...          ...         ...   \n",
       "2222     0.0  0.000000  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "2223     0.0  0.000000  ...   0.0   0.0  0.234011          0.0    0.275489   \n",
       "2224     0.0  0.000000  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "2225     0.0  0.000000  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "2226     0.0  0.573535  ...   0.0   0.0  0.000000          0.0    0.000000   \n",
       "\n",
       "      yep  yes  yes gowalla  yet  yrs  \n",
       "0     0.0  0.0          0.0  0.0  0.0  \n",
       "1     0.0  0.0          0.0  0.0  0.0  \n",
       "2     0.0  0.0          0.0  0.0  0.0  \n",
       "3     0.0  0.0          0.0  0.0  0.0  \n",
       "4     0.0  0.0          0.0  0.0  0.0  \n",
       "...   ...  ...          ...  ...  ...  \n",
       "2222  0.0  0.0          0.0  0.0  0.0  \n",
       "2223  0.0  0.0          0.0  0.0  0.0  \n",
       "2224  0.0  0.0          0.0  0.0  0.0  \n",
       "2225  0.0  0.0          0.0  0.0  0.0  \n",
       "2226  0.0  0.0          0.0  0.0  0.0  \n",
       "\n",
       "[2227 rows x 1252 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
